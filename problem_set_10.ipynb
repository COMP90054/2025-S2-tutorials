{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/COMP90054/2025-S2-tutorials/blob/main/problem_set_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtYhEJOLntKg"
      },
      "source": [
        "# COMP90054 AI Planning for Autonomy\n",
        "### Problem Set 10\n",
        " - Policy approximation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzM1qCxE1yDN"
      },
      "source": [
        "### Key concepts:\n",
        "- Policy approximation\n",
        "- Policy gradients\n",
        "- Actor critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8f4i2S328SJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Problem 1:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE3VuWKWYD4R"
      },
      "source": [
        "Discuss the following questions in groups:\n",
        "\n",
        "- What is the difference between these different RL methods: \n",
        "    - Policy approximation \n",
        "    - Policy gradient \n",
        "    - Actor critic \n",
        "\n",
        "- What is the difference between these optimisation algorithms: \n",
        "    - Gradient descent \n",
        "    - Stochastic gradient descent \n",
        "    - Approximate gradient descent \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAVg7eLm1K8M"
      },
      "source": [
        "### Problem 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "Recall the grid world from last lecture (details below). Now instead of doing linear-SARSA, Andrew the average computer scientist is trying to use a policy-gradient method. He has decided to use linear-softmax using the same features as last time. \n",
        "1. Assume weights of [0.1, -0.2]. What is the probability of taking each action from the state (2,3)?\n",
        "2. Does using a policy gradient method improve upon the issue of having a goal in the middle of the board? Does it solve it?\n",
        "3. After moving up from (2,3), the agent reaches the goal 25 timesteps later. Calculate the change in weights from this transition assuming the REINFORCE algorithm (MC policy gradient) is used. \n",
        "4. What can you say about the weight updates in general for this problem using REINFORCE with no baseline term?\n",
        "5. (Bonus) Recalculate the weight update from question 3 using Advantage-Actor Critic instead of REINFORCE, assume the value function uses the same features and currently has weights of [0.05, 0.03].\n",
        "\n",
        "\n",
        "\n",
        "Environment recap: \n",
        "Consider a simple MDP consisting of a 10x10 grid, the agent can move up, down, left, or right. The agent starts in (0,0) and gets a reward of +1 when reaching (9,9) which is a terminal state. $\\gamma = 0.9$, and there is no randomness. Andrew the average computer scientist has designed the following feature representation: $f((x,y),A) =  (x',y')$, where $x'$ and $y'$ are the coordinates of the agent after applying action $A$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_x1EZbMXwbg"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### Problem 3 (optional - not covered in class):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the following questions:\n",
        "- Why are value-based RL methods inappropriate for continuous action spaces? How could you apply them anyway? How do policy-based methods address this challenge instead?\n",
        "- What are the two major changes PPO makes from A2C? How do these \n",
        "- What is the purpose of the entropy term in the loss function in PPO or other policy gradient methods?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HJLiN8UdxEF-"
      ],
      "include_colab_link": true,
      "name": "problem_set_07.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
