{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/COMP90054/2025-S2-tutorials/blob/main/problem_set_09.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtYhEJOLntKg"
      },
      "source": [
        "# COMP90054 AI Planning for Autonomy\n",
        "### Problem Set 09\n",
        " - Value Function Approximation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzM1qCxE1yDN"
      },
      "source": [
        "### Key concepts:\n",
        "- The role of function approximators in RL\n",
        "- Gradient descent\n",
        "- DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8f4i2S328SJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Problem 1:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE3VuWKWYD4R"
      },
      "source": [
        "Discuss the following questions in groups:\n",
        "\n",
        "- What is the primary benefit of function approximation in RL?\n",
        "- What problem is batch RL trying to solve?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAVg7eLm1K8M"
      },
      "source": [
        "### Problem 2:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "Consider a simple MDP consisting of a 10x10 grid, the agent can move up, down, left, or right. The agent starts in (0,0) and gets a reward of +1 when reaching (9,9) which is a terminal state. $\\gamma = 0.9$, and there is no randomness. \n",
        "\n",
        "Andrew the average computer scientist has designed the following feature representation: $\\hat{q}((x,y),A, \\mathbf{w}) = w_0\\cdot x' + w_1 \\cdot y'$, where $x'$ and $y'$ are the coordinates of the agent after applying action $A$. \n",
        "He is trying to perform incremental control with action-value function approximation using TD(0), i.e. linear Q-learning. Imagine the agent moves up from the cell (2,3). The existing weight vector is [0.1,-0.2]. \n",
        "\n",
        "1. Perform a single weight update from that transition with $\\alpha = 0.2$. Recall from the lecture slides, the weight update equation is: $\\Delta \\mathbf{w} = \\alpha ({R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\mathbf{w})} - \\hat{q}(S_t, A_t, \\mathbf{w})) \\nabla_\\mathbf{w} \\hat{q}(S_t, A_t, \\mathbf{w})$\n",
        "2. What weights would result in an optimal greedy policy?\n",
        "3. Consider changing the MDP so that the terminal reward state is in (5,5). What problems does this cause?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_x1EZbMXwbg"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### Problem 3:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Go to Andrej Kaparthy's [DQN demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html). Read the description of the MDP, and observe the agent learning. \n",
        "\n",
        "Now, consider the following questions:\n",
        " - How does the given state representation differ from the image you are seeing. What would be the implications of changing to a pixel based representation?\n",
        " - Does the current state representation fully describe the entire environment? Is this an MDP?\n",
        " - Try to verbally describe a reasonable policy for this problem (in terms of the current state representation). This will be necessarily be a bit vague.\n",
        " - Imagine you are using linear feature approximation. What feature weights could correspond to the policy you described above? Are there issues with this? How does using a neural network address this?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HJLiN8UdxEF-"
      ],
      "include_colab_link": true,
      "name": "problem_set_07.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
